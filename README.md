## Dear Recruiters,  

My name is **Cao Ngoc Hong Anh**, a fourth-year student majoring in International Business. During my studies, I discovered a strong passion for **technology and coding**, which motivated me to challenge myself with my very first coding project: **a Simple Data Pipeline (ETL)**.  

Although I do not come from a Computer Science background, I wanted to **prove to myself** that I could learn coding logic, debug errors, and document technical work in a structured way. This project represents the starting point of my journey into technology—and I hope it also shows my potential to grow into the Tech World.  

---

## Project Title: Designing a Simple Data Pipeline Project  

### Abstract  
This project simulates the **Extract → Transform → Load (ETL)** process, where raw sales data is cleaned and organized into a ready-to-use format. While it does not directly use ABAP, the skills I practiced: **modular coding, error handling, debugging, and clear documentation** mirror the daily responsibilities of an ABAP developer.  

---

## Project Structure  

I organized my project into clear modules because I wanted to **think like a developer** rather than just write one long script:  
```
data/
├── processed_sales.csv
├── sales.csv
extract.py
load.py
transform.py
main.py
etl.log
```

- **sales.csv** → raw data  
- **extract.py** → safe data extraction  
- **transform.py** → cleaning & calculations  
- **load.py** → export clean data  
- **main.py** → orchestrates everything  
- **etl.log** → execution + error logs  
- **processed_sales.csv** → export clean data
---

## The Journey  

I started with a simple sales dataset (`sales.csv`) that contained order information: Order ID, Product, Quantity, Price, and Date (This is generated by ChatGPT).  

- In **extract.py**, I wrote a function to safely load the CSV into Python, while checking if the file existed. This was my way of practicing **error prevention**.  
- In **transform.py**, I created a new calculated column, **“Total = Quantity × Price”**, and standardized the date formats. This step reminded me how **data consistency** is essential before generating reports.  
- In **load.py**, I ensured the cleaned data was saved into a new CSV file, creating folders automatically if missing. This taught me the importance of **automation and reliability**.  
- Finally, in **main.py**, I tied everything together. This script runs the entire ETL process while also generating a log file (`etl.log`) to track progress. Seeing the logs print **“Starting ETL process… Transformation complete… Data loaded successfully”** gave me a real taste of debugging.  

---

## What I Learned  

This was not just about writing Python code—it was about building the **mindset of a developer**.  

- I practiced **debugging** when the input file was missing or when formats caused errors.  
- I learned the value of **structured coding**, where each function does one job but integrates with others.  
- I developed **patience and persistence**, because every small bug required logical problem-solving.  
- I realized coding demands the same qualities as consulting: **problem-solving, detail orientation, and clear communication**.  

---

## Reflection  

I know this project is still **very simple**: the dataset is small, and I relied on ChatGPT for coding guidance as I am still a beginner. But the **ideas, structure, and logic** were designed by me.  

To me, this project is proof of my **curiosity and willingness to learn**. At EY, I want to bring that same energy to ABAP development—learning quickly, solving problems, and contributing to client success.  

---

## Conclusion  

This Data Pipeline Project is my **first step into coding**. It may be small, but it carries a bigger meaning: my determination to **learn beyond my major**, my ability to **adapt to new tools**, and my motivation to apply these skills in the real world.  

I look forward to continuing this journey with EY, where I can transform this curiosity into **professional expertise in ABAP consulting**.  

